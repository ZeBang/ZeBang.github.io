<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-GB">
   <head>
      <title>CP</title>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <style type="text/css">
         @import url('../style/style.css');
         p.metadata {
         font-size: 100%;
         color: #999;
         margin: 0;
         }
      </style>
      
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: { equationNumbers: {autoNumber: "all"} },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
    });
</script>
		<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

   </head>
   <body>
      <h1>CP decomposition</h1>
      <div id="contact">
         <p>
            <br>
            Some facts and properties of CP decomposition are collected. Since when dimention of data array goes larger than two things changed fundamentally from matrix, these facts and progress on tensor is necessary to be noticed. <br>
            <ul><li>Low-rank approximation,</li></ul>
            <ul><li>Identification of CP Decomposition, </li></ul>
            <ul><li>Notions of orthogonality. </li></ul>
         </p>
      </div>
      <!-- contact -->
      <div id="photo">
        <br>
         <img style="width: 200px;" alt="algorithm" src="../images/tensor.png" align="middle"/>
      </div>
      <div style="clear: both;" id="main">
         <h3>Notation</h3>
<p>An \(K\)th-order rank-1 tensor \(\mathcal{X} \in \mathbb{C}^{I_1 \times \cdots \times I_K}\) is defined as the tensor product of nonzero vectors \(\textbf{a}^{(k)} \in \mathbb{C}^{I_k} , 1 \le k \le K\), such that \(\mathcal{X}_{i_1 \cdots i_K} =  \prod_{k=1}^{K} \textbf{a}_{i_k}^{k}\). We write \(\mathcal{X} = \textbf{a}^{(1)} \circ \textbf{a}^{(2)} \circ \cdots \circ \textbf{a}^{(K)}\). The rank of a tensor \(\mathcal{X}\) is equal to the minimal number of rank-1 tensors that yield \(\mathcal{X}\) in a linear combination. Assume that the rank of \(\mathcal{X}\) is \(R\); then it can be written as
\begin{equation}\label{cp}
    \mathcal{X} = \sum_{r=1}^{R} \textbf{a}_r^{(1)} \circ \cdots \circ \textbf{a}_r^{(K)}
\end{equation}
where $\textbf{a}_r^{(k)} \in \mathbb{C}^{I_k}$. Let us stack the vectors $\textbf{a}_{r}^{(k)}$ into the matrices
\begin{equation}\label{factor}
    \mathbf{A}^{(k)} = \Big[\mathbf{a}_1^{(k)}, \cdots, \mathbf{a}_R^{(k)} \Big] \in \mathbb{C}^{I_k \times R}, 1 \le k \le K
\end{equation}
The matrices \(\mathbf{A}^{(k)}\) will be denoted as factor matrices.</p>


  <h3>Definition of Uniqueness</h3>
<p>The CP Decomposition of a tensor is unique if all the factor matrices $\mathbf{\bar{A}}^{(1)},\cdots, \mathbf{\bar{A}}^{(K)}$ satisfying (\ref{factor}) are related via
\begin{equation}
    \mathbf{\bar{A}}^{(k)} = \mathbf{A}^{(k)} \mathbf{P} \Delta
\end{equation}
where $\Delta$ are diagonal matrices satisfying $\prod_{k=1}^{K} \Delta _{\mathbf{A}^{(k)}} = \mathbf{I}_{R}$ and $\mathbf{P}$ is a permutation matrix.</p>



         <!-- <h2>Papers</h2>
         <p id="publications">


        <ul>
            <li>
               H. Deng, Q. Han and C.-H. Zhang  (2020)  Confidence intervals for multiple isotonic regression and other monotone models. [<a target="_blank" href="https://arxiv.org/abs/2001.07064">arXiv</a>]

            </li>
         </ul>



        <ul>
            <li>
               Q. Han and K. Kato  (2019)  Berry-Esseen bounds for Chernoff-type non-standard asymptotics in isotonic regression.  [<a target="_blank" href="https://arxiv.org/abs/1910.09662">arXiv</a>]

            </li>
         </ul>



         <ul>
            <li>
               Q. Han (2019)  Global empirical risk minimizers with “shape constraints” are rate optimal in general dimensions.  [<a target="_blank" href="https://arxiv.org/abs/1905.12823?context=math.ST">arXiv</a>]
            </li>
         </ul>

        <ul>
            <li>
               Q. Han (2019)  Multiplier U-processes: sharp bounds and applications.  [<a target="_blank" href=pdf/multiplier_U_processes_v1.pdf>pdf</a>]
            </li>
         </ul>


 
         <ul>
            <li>
               Q. Han and J. A. Wellner  (2019)  Complex sampling designs: uniform limit theorems and applications. [<a target="_blank" href="https://arxiv.org/abs/1905.12824?context=math.ST">arXiv</a>]
            </li>
         </ul>


         <ul>
            <li>
               Q. Han and C.-H. Zhang  (2019+)  Limit distribution theory for block estimators in multiple isotonic regression. <i>Ann. Statist.</i>, to appear. [<a target="_blank" href="https://arxiv.org/abs/1905.12825">arXiv</a>]
            </li>
         </ul>



         <ul>
            <li>
               Q. Han and J. A. Wellner  (2018)  Robustness of shape-restricted regression estimators: an envelope perspective.   [<a target="_blank" href="https://arxiv.org/abs/1805.02542">arXiv</a>]
            </li>
         </ul>


         <ul>
            <li>
              Q. Han*, T. Wang*, S. Chatterjee and R. J. Samworth  (2019)  Isotonic regression in general dimensions. <i>Ann. Statist.</i>, <b>47</b>, 2440-2471. [<a target="_blank" href="https://arxiv.org/abs/1708.09468">arXiv</a>] (*=equal contribution)
            </li>
         </ul>


         <ul>
            <li>
               Q. Han and J. A. Wellner  (2019)  Convergence rates of least squares regression estimators with heavy-tailed errors. <i>Ann. Statist.</i>, <b>47</b>, 2286&ndash;2319. [<a target="_blank" href="https://arxiv.org/abs/1706.02410">arXiv</a>] <br/>
<small> (Presented in the Annals of Statistics special invited session at JSM 2019) </small>
            </li>
         </ul>

         <ul>
            <li>
               Q. Han (2017)  Oracle posterior contraction rates under hierarchical priors.  [<a target="_blank" href="https://arxiv.org/abs/1704.07513">arXiv</a>]             </li>
         </ul>         
         <ul>
            <li>
           Q. Han and J. A. Wellner  (2016)  Multivariate convex regression: global risk bounds and adaptation.  
               [<a target="_blank" href="https://arxiv.org/abs/1601.06844">arXiv</a>]
            </li>
         </ul>

   <ul>
   <li>
          Q. Han and J. A. Wellner  (2016)  Approximation and estimation of s-concave densities via R&eacutenyi divergences. <i>Ann. Statist.</i>, <b>44</b>, 1332&ndash;1359.  [<a target="_blank" href="https://arxiv.org/abs/1505.00379">arXiv</a>]
   </li>
   </ul> 

   <ul>
   <li>  A. Jalali, Q. Han, I. Dumitriu and M. Fazel (2016) Relative density and exact recovery in general stochastic block models. <i> NIPS 2016.</i> [<a target="_blank" href="https://arxiv.org/abs/1512.04937">arXiv</a>]

   </li>
   </ul>
         </p>


         <h2>Upcoming travel</h2>
         <p id="teaching">
         <ul>
            <li>
              Statistics seminar. Mar 5, 2020. Department of Statistics, University of Minnesota, USA.  
            </li>
         </ul>
         <ul>
            <li>
              Statistics seminar. May 21, 2020. Department of Statistics, University of California Davis, USA.  
            </li>
         </ul>
         <ul>
            <li>
              2020 ICSA Conference. Jun 26-Jun 29, 2020. Wuhan, China. 
            </li>
         </ul>
        <ul>
            <li>
              Mathematical and Statistical Challenges in Uncertainty Quantification. Jul 13-16, 2020. Cambridge, UK.
            </li>
         </ul>
        <ul>
            <li>
              2020 IMS/Bernoulli World Congress of Probability and Statistics. Aug 17-21, 2020, Seoul, South Korea.
            </li>
         </ul>
        <ul>
            <li>
              2020 Workshop New Developments in Econometrics and Time Series. Oct 1-2, 2020, Renne, France.
            </li>
         </ul>

            </p>



       <h2>Teaching</h2>
         <p id="teaching">

       <ul>
            <li>
              Instructor, <a target="_blank" href=STAT652/STAT652.html>STAT 652-653</a>, Rutgers, Fall 2019/Spring 2020.
            </li>
         </ul>

         <ul>
            <li>
              Instructor, STAT 653, Rutgers, Spring 2019.
            </li>
         </ul>
         <ul>
            <li>
              Instructor, STAT 593, Rutgers, Fall 2018.
            </li>
         </ul>
         <ul>
            <li>
              Instructor, STAT/MATH 491, UW, Fall 2017.
            </li>
         </ul>
            </p>



      </div> -->
      <!--main-->
   </body>
</html>